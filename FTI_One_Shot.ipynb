{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a0f803-46da-439f-8634-6bbb89303411",
   "metadata": {},
   "source": [
    "# One Shot Learning For File Type Identification\n",
    "## Student ID1: 313260382\n",
    "## Student ID2: 316305531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a480bb73-cd79-46d7-a6ce-a45ec90fd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Concatenate, Dot, Lambda, Input\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f910f2-e834-4f64-b295-be870206e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: x.shape=(768000, 512), y.shape=(768000,)\n"
     ]
    }
   ],
   "source": [
    "#load data:\n",
    "def load(scenario=1, block_size='4k', subset='train'):\n",
    "  if block_size not in ['512']:\n",
    "    raise ValueError('Invalid block size!')\n",
    "  if scenario not in range(1 ,6):\n",
    "    raise ValueError('Invalid scenario!')\n",
    "  if subset not in ['train', 'val', 'test']:\n",
    "    raise ValueError('Invalid subset!')\n",
    "\n",
    "  data_dir = os.path.join('.', '{:s}_{:1d}'.format(block_size, scenario))\n",
    "  data = np.load(os.path.join(data_dir, '{}.npz'.format(subset)))\n",
    " \n",
    "  if os.path.isfile('classes.json'):\n",
    "    with open('classes.json') as json_file:\n",
    "      classes = json.load(json_file)\n",
    "      labels = classes[str(scenario)]\n",
    "  else:\n",
    "    raise FileNotFoundError('Please download classes.json to the current directory!')\n",
    "\n",
    "  return data['x'], data['y'], labels\n",
    "\n",
    "x, y, l = load(1, '512', 'test')\n",
    "xv, yv, lv = load(1, '512', 'val')\n",
    "\n",
    "print(\"Loaded data: x.shape={}, y.shape={}\".format(x.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6579f5b0-c84e-41a9-9d97-d1baf8baa0b4",
   "metadata": {},
   "source": [
    "# List of Filetypes\n",
    "The classifier has been tested on the following 75 filetypes:<br>\n",
    "11 types & 75 subtypes\n",
    "### RAW\n",
    "ARW, \n",
    "CR2, \n",
    "DNG, \n",
    "GPR, \n",
    "NEF, \n",
    "NRW, \n",
    "ORF, \n",
    "PEF, \n",
    "RAF, \n",
    "RW2, \n",
    "3FR\n",
    "### Bitmap\n",
    "JPG, \n",
    "TIFF,\n",
    "HEIC,\n",
    "BMP, \n",
    "GIF, \n",
    "PNG\n",
    "### Vector\n",
    "AI,\n",
    "EPS,\n",
    "PSD,\n",
    "### Video\n",
    "MOV,\n",
    "MP4,\n",
    "3GP,\n",
    "AVI,\n",
    "MKV,\n",
    "OGV,\n",
    "WEBM,\n",
    "### Archive\n",
    "APK,\n",
    "JAR,\n",
    "MSI,\n",
    "DMG,\n",
    "7Z,\n",
    "BZ2,\n",
    "DEB,\n",
    "GZ,\n",
    "PKG,\n",
    "RAR,\n",
    "RPM,\n",
    "XZ,\n",
    "ZIP\n",
    "### Executables\n",
    "EXE,\n",
    "MACH-O,\n",
    "ELF,\n",
    "DLL\n",
    "### Office\n",
    "DOC,\n",
    "DOCX,\n",
    "KEY,\n",
    "PPT,\n",
    "PPTX,\n",
    "XLS,\n",
    "XLSX\n",
    "### Published\n",
    "DJVU,\n",
    "EPUB,\n",
    "MOBI,\n",
    "PDF\n",
    "### Human-readable\n",
    "MD,\n",
    "RTF,\n",
    "TXT,\n",
    "TEX,\n",
    "JSON,\n",
    "HTML,\n",
    "XML,\n",
    "LOG,\n",
    "CSV,\n",
    "### Audio\n",
    "AIFF,\n",
    "FLAC,\n",
    "M4A,\n",
    "MP3,\n",
    "OGG,\n",
    "WAV,\n",
    "WMA\n",
    "### Other\n",
    "PCAP,\n",
    "TTF,\n",
    "DWG,\n",
    "SQLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5193df0f-1b12-44a4-b074-66094911f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of each types:\n",
    "def oneshot(x,y):\n",
    "    ls=[i for i in range(75)]\n",
    "    ls_x= []\n",
    "    ls_y= []\n",
    "    for index_x , index_y in zip(x,y):\n",
    "        if index_y in ls:\n",
    "            if index_y == 0:\n",
    "                ls_x.append(index_x)\n",
    "                ls_y.append(1)\n",
    "                ls.remove(index_y)\n",
    "            else:\n",
    "                ls_x.append(index_x)\n",
    "                ls_y.append(0)\n",
    "                ls.remove(index_y)\n",
    "    return np.asarray(ls_x), np.asarray(ls_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f4ba37-0c94-4735-8f36-f2e24f85cfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARKUlEQVR4nO3df6zddX3H8edrrcOCq4JcSG2btWYdWpoJ0tQ6FuPEjfojlj9k1kRpFpYmpE5cTLTdkrn90YQli1HiIGlEKdPAOnSjcUNlVbJsYbDLDweldnTC6B2VXn8Tl6HF9/44n8rx9rS099ze8y33+Ui+Od/v+3w/3/u+95x7X/1+vuecpqqQJOmXRt2AJKkbDARJEmAgSJIaA0GSBBgIkqRm/qgbmK5zzz23li1bNuo2JOm0cv/993+nqsYG3XfaBsKyZcsYHx8fdRuSdFpJ8t/Hus8pI0kSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJwGr9TWXohy7b8wy9sP3HdO05qzInsL8GL53ljIDC9PxzHO8ag8Sf7hBn2CTb1e5pqNr7Hrv2SDHqcT/XjNuqfwYk8t7vUYxeeJ4N07XE9VQyEU+TF+oQZxkwHyih+xsP+Yehi0L8YzfbjMuzxX+hxnepUPc4GwgnyD/zR/JmcesP8YZuprznT28P0cyJ8Lk7fCwZCks8A7wQOVdWqVjsH+BtgGfAE8HtV9f1231bgauA54INV9ZVWvwS4GVgA/CNwbVVVkjOAW4BLgO8C76mqJ2bsOxzghZ5gs3GaPdP/0pzpX7oX82nxXDfqx7WLZ35d7GEUTuQM4WbgU/T+aB+xBdhdVdcl2dK2P5pkJbABuBB4FfBPSX69qp4DbgQ2Af9GLxDWAXfSC4/vV9WvJdkA/AXwnpn45l5MXoxP0Jn+nmbiX8ej9mJ8nOei0yH0BnnBQKiqf06ybEp5PfDmtr4DuBv4aKvfVlXPAo8n2Q+sSfIEsLCq7gFIcgtwBb1AWA/8WTvW7cCnkqSqarrflF7Y6fjHs6u/ROq+uXJReFjTvYZwflUdBKiqg0nOa/XF9M4AjphotZ+29an1I2MOtGMdTvJD4JXAd6bZm3Ta8A+TumSm35iWAbU6Tv14Y44+eLIpyXiS8cnJyWm2KEkaZLqB8HSSRQDt9lCrTwBL+/ZbAjzV6ksG1H9hTJL5wMuB7w36olW1vapWV9XqsbGB/yWoJGmaphsIu4CNbX0jcEdffUOSM5IsB1YA97XppWeSrE0S4KopY44c693A17x+IEmz70RednorvQvI5yaZAD4GXAfsTHI18CRwJUBV7UmyE3gUOAxsbq8wAriG5192emdbAG4C/rpdgP4evVcpSZJm2Ym8yui9x7jrsmPsvw3YNqA+DqwaUP8/WqBIkkbHTzuVJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBAwZCEn+KMmeJI8kuTXJS5Ock+SuJI+127P79t+aZH+SfUku76tfkuThdt/1STJMX5KkkzftQEiyGPggsLqqVgHzgA3AFmB3Va0Adrdtkqxs918IrANuSDKvHe5GYBOwoi3rptuXJGl6hp0ymg8sSDIfOBN4ClgP7Gj37wCuaOvrgduq6tmqehzYD6xJsghYWFX3VFUBt/SNkSTNkmkHQlX9D/CXwJPAQeCHVfVV4PyqOtj2OQic14YsBg70HWKi1Ra39an1oyTZlGQ8yfjk5OR0W5ckDTDMlNHZ9P7Vvxx4FXBWkvcdb8iAWh2nfnSxantVra6q1WNjYyfbsiTpOIaZMnor8HhVTVbVT4EvAr8JPN2mgWi3h9r+E8DSvvFL6E0xTbT1qXVJ0iwaJhCeBNYmObO9KugyYC+wC9jY9tkI3NHWdwEbkpyRZDm9i8f3tWmlZ5Ksbce5qm+MJGmWzJ/uwKq6N8ntwAPAYeBBYDvwMmBnkqvphcaVbf89SXYCj7b9N1fVc+1w1wA3AwuAO9siSZpF0w4EgKr6GPCxKeVn6Z0tDNp/G7BtQH0cWDVML5Kk4fhOZUkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEnAkIGQ5BVJbk/yzSR7k7wxyTlJ7kryWLs9u2//rUn2J9mX5PK++iVJHm73XZ8kw/QlSTp5w54hfBL4clW9BngdsBfYAuyuqhXA7rZNkpXABuBCYB1wQ5J57Tg3ApuAFW1ZN2RfkqSTNO1ASLIQeBNwE0BV/aSqfgCsB3a03XYAV7T19cBtVfVsVT0O7AfWJFkELKyqe6qqgFv6xkiSZskwZwivBiaBzyZ5MMmnk5wFnF9VBwHa7Xlt/8XAgb7xE622uK1PrR8lyaYk40nGJycnh2hdkjTVMIEwH3g9cGNVXQz8mDY9dAyDrgvUcepHF6u2V9Xqqlo9NjZ2sv1Kko5jmECYACaq6t62fTu9gHi6TQPRbg/17b+0b/wS4KlWXzKgLkmaRdMOhKr6NnAgyQWtdBnwKLAL2NhqG4E72vouYEOSM5Isp3fx+L42rfRMkrXt1UVX9Y2RJM2S+UOO/0Pg80l+GfgW8Pv0QmZnkquBJ4ErAapqT5Kd9ELjMLC5qp5rx7kGuBlYANzZFknSLBoqEKrqIWD1gLsuO8b+24BtA+rjwKphepEkDcd3KkuSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCZiAQksxL8mCSL7Xtc5LcleSxdnt2375bk+xPsi/J5X31S5I83O67PkmG7UuSdHJm4gzhWmBv3/YWYHdVrQB2t22SrAQ2ABcC64AbksxrY24ENgEr2rJuBvqSJJ2EoQIhyRLgHcCn+8rrgR1tfQdwRV/9tqp6tqoeB/YDa5IsAhZW1T1VVcAtfWMkSbNk2DOETwAfAX7WVzu/qg4CtNvzWn0xcKBvv4lWW9zWp9aPkmRTkvEk45OTk0O2LknqN+1ASPJO4FBV3X+iQwbU6jj1o4tV26tqdVWtHhsbO8EvK0k6EfOHGHsp8K4kbwdeCixM8jng6SSLqupgmw461PafAJb2jV8CPNXqSwbUJUmzaNpnCFW1taqWVNUyeheLv1ZV7wN2ARvbbhuBO9r6LmBDkjOSLKd38fi+Nq30TJK17dVFV/WNkSTNkmHOEI7lOmBnkquBJ4ErAapqT5KdwKPAYWBzVT3XxlwD3AwsAO5siyRpFs1IIFTV3cDdbf27wGXH2G8bsG1AfRxYNRO9SJKmx3cqS5IAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQKGCIQkS5N8PcneJHuSXNvq5yS5K8lj7fbsvjFbk+xPsi/J5X31S5I83O67PkmG+7YkSSdrmDOEw8CHq+q1wFpgc5KVwBZgd1WtAHa3bdp9G4ALgXXADUnmtWPdCGwCVrRl3RB9SZKmYdqBUFUHq+qBtv4MsBdYDKwHdrTddgBXtPX1wG1V9WxVPQ7sB9YkWQQsrKp7qqqAW/rGSJJmyYxcQ0iyDLgYuBc4v6oOQi80gPPabouBA33DJlptcVufWh/0dTYlGU8yPjk5OROtS5KaoQMhycuALwAfqqofHW/XAbU6Tv3oYtX2qlpdVavHxsZOvllJ0jENFQhJXkIvDD5fVV9s5afbNBDt9lCrTwBL+4YvAZ5q9SUD6pKkWTTMq4wC3ATsraqP9921C9jY1jcCd/TVNyQ5I8lyeheP72vTSs8kWduOeVXfGEnSLJk/xNhLgfcDDyd5qNX+GLgO2JnkauBJ4EqAqtqTZCfwKL1XKG2uqufauGuAm4EFwJ1tkSTNomkHQlX9C4Pn/wEuO8aYbcC2AfVxYNV0e5EkDc93KkuSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCOhQISdYl2Zdkf5Ito+5HkuaaTgRCknnAXwFvA1YC702ycrRdSdLc0olAANYA+6vqW1X1E+A2YP2Ie5KkOSVVNeoeSPJuYF1V/UHbfj/whqr6wJT9NgGb2uYFwL4hv/S5wHeGPMap1vUeu94f2ONM6Hp/YI8n6leramzQHfNnu5NjyIDaUUlVVduB7TP2RZPxqlo9U8c7FbreY9f7A3ucCV3vD+xxJnRlymgCWNq3vQR4akS9SNKc1JVA+HdgRZLlSX4Z2ADsGnFPkjSndGLKqKoOJ/kA8BVgHvCZqtozC196xqafTqGu99j1/sAeZ0LX+wN7HFonLipLkkavK1NGkqQRMxAkScAcDYQufkxGks8kOZTkkb7aOUnuSvJYuz17xD0uTfL1JHuT7ElybZf6TPLSJPcl+Ubr78+71N+UXucleTDJl7rYY5Inkjyc5KEk413rMckrktye5Jvt+fjGjvV3QfvZHVl+lORDXepxkDkXCB3+mIybgXVTaluA3VW1AtjdtkfpMPDhqnotsBbY3H52XenzWeAtVfU64CJgXZK1Heqv37XA3r7tLvb421V1Ud/r5rvU4yeBL1fVa4DX0ftZdqa/qtrXfnYXAZcA/wv8XZd6HKiq5tQCvBH4St/2VmDrqPtqvSwDHunb3gcsauuLgH2j7nFKv3cAv9PFPoEzgQeAN3StP3rvs9kNvAX4Uhcfa+AJ4NwptU70CCwEHqe9KKZr/Q3o93eBf+1yj0eWOXeGACwGDvRtT7RaF51fVQcB2u15I+7n55IsAy4G7qVDfbapmIeAQ8BdVdWp/ppPAB8BftZX61qPBXw1yf3tI2OgOz2+GpgEPtum3T6d5KwO9TfVBuDWtt7VHoE5OGXECX5Mho4tycuALwAfqqofjbqfflX1XPVO05cAa5KsGnFLvyDJO4FDVXX/qHt5AZdW1evpTa1uTvKmUTfUZz7weuDGqroY+DFdm3pp2htt3wX87ah7ORFzMRBOp4/JeDrJIoB2e2jE/ZDkJfTC4PNV9cVW7lyfVfUD4G5612W61N+lwLuSPEHvU33fkuRzdKtHquqpdnuI3tz3GrrT4wQw0c7+AG6nFxBd6a/f24AHqurptt3FHn9uLgbC6fQxGbuAjW19I705+5FJEuAmYG9Vfbzvrk70mWQsySva+gLgrcA3u9IfQFVtraolVbWM3nPva1X1PjrUY5KzkvzKkXV6c+CP0JEeq+rbwIEkF7TSZcCjdKS/Kd7L89NF0M0enzfqixijWIC3A/8J/BfwJ6Pup/V0K3AQ+Cm9fwFdDbyS3sXHx9rtOSPu8bfoTa/9B/BQW97elT6B3wAebP09Avxpq3eivwH9vpnnLyp3pkd6c/TfaMueI78jHevxImC8PdZ/D5zdpf5aj2cC3wVe3lfrVI9TFz+6QpIEzM0pI0nSAAaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLU/D/zLmU4MPH1HwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unique, counts = np.unique(y, return_counts=True)\n",
    "# D=dict(zip(unique, counts))\n",
    "# plt.bar(*zip(*D.items()))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15dd61f5-8285-48c6-8a0c-8d9ba9a57c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data \n",
    "(x_train, y_train) = oneshot(x, y)\n",
    "(x_test, y_test) = xv[:285], yv[:285]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41acebae-fb39-442b-aecd-8809d3cb7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pairs:\n",
    "def make_pairs(x, y):\n",
    "    num_classes = max(y) + 1\n",
    "    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
    "\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for idx1 in range(len(x)):\n",
    "        # add a matching example\n",
    "        x1 = x[idx1]\n",
    "        label1 = y[idx1]\n",
    "        idx2 = random.choice(digit_indices[label1])\n",
    "        x2 = x[idx2]\n",
    "        \n",
    "        pairs += [[x1, x2]]\n",
    "        labels += [1]\n",
    "    \n",
    "        # add a not matching example\n",
    "        label2 = random.randint(0, num_classes-1)\n",
    "        while label2 == label1:\n",
    "            label2 = random.randint(0, num_classes-1)\n",
    "\n",
    "        idx2 = random.choice(digit_indices[label2])\n",
    "        x2 = x[idx2]\n",
    "        \n",
    "        pairs += [[x1, x2]]\n",
    "        labels += [0]\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "pairs_train, labels_train = make_pairs(x_train, y_train)\n",
    "pairs_test, labels_test = make_pairs(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f31d1c9-fe0f-410c-98da-db4028eac656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "input = Input((1,512))\n",
    "n = Flatten()(input)\n",
    "n = Dense(128, activation='relu')(n)\n",
    "dense = Model(input, n)\n",
    "\n",
    "input1 = Input((1,512))\n",
    "input2 = Input((1,512))\n",
    "\n",
    "dense1 = dense(input1)\n",
    "dense2 = dense(input2)\n",
    "\n",
    "merge_layer = Lambda(euclidean_distance)([dense1,dense2])\n",
    "dense_layer = Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "model = Model(inputs=[input1, input2], outputs=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a332609c-f240-4f8f-ba1b-dee9d32b050c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 512)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1, 512)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, 128)          65664       input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           functional_1[0][0]               \n",
      "                                                                 functional_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 65,666\n",
      "Trainable params: 65,666\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e201cc41-ac7d-4595-b517-acaf106a52cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2dizz80f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GFLOPs</td><td>0.00013</td></tr><tr><td>accuracy</td><td>0.7</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>loss</td><td>0.54108</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">peachy-snow-52</strong>: <a href=\"https://wandb.ai/projfft/siamese/runs/2dizz80f\" target=\"_blank\">https://wandb.ai/projfft/siamese/runs/2dizz80f</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220816_185027-2dizz80f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2dizz80f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Orkabi\\Desktop\\cs_ariel\\FTI\\wandb\\run-20220816_185223-18vmzp3f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/projfft/siamese/runs/18vmzp3f\" target=\"_blank\">drawn-snowball-53</a></strong> to <a href=\"https://wandb.ai/projfft/siamese\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 512) for input Tensor(\"input_2:0\", shape=(None, 1, 512), dtype=float32), but it was called on an input with incompatible shape (75, 512).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 512) for input Tensor(\"input_3:0\", shape=(None, 1, 512), dtype=float32), but it was called on an input with incompatible shape (75, 512).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 512) for input Tensor(\"input_1:0\", shape=(None, 1, 512), dtype=float32), but it was called on an input with incompatible shape (75, 512).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 512) for input Tensor(\"input_1:0\", shape=(None, 1, 512), dtype=float32), but it was called on an input with incompatible shape (75, 512).\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5428 - accuracy: 0.7000\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5420 - accuracy: 0.7000\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5428 - accuracy: 0.7000\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5422 - accuracy: 0.7000\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5419 - accuracy: 0.7000\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5410 - accuracy: 0.7000\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5409 - accuracy: 0.7000\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5415 - accuracy: 0.7000\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5405 - accuracy: 0.7000\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5395 - accuracy: 0.7000\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5402 - accuracy: 0.7000\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5397 - accuracy: 0.7067\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5399 - accuracy: 0.7000\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5397 - accuracy: 0.7000\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5393 - accuracy: 0.7000\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5386 - accuracy: 0.7000\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5386 - accuracy: 0.7000\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5385 - accuracy: 0.7000\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5387 - accuracy: 0.7067\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5384 - accuracy: 0.7067\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5373 - accuracy: 0.7067\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5377 - accuracy: 0.7000\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5371 - accuracy: 0.7000\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5369 - accuracy: 0.7000\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5371 - accuracy: 0.7000\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5367 - accuracy: 0.7000\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5364 - accuracy: 0.7067\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5361 - accuracy: 0.7067\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5361 - accuracy: 0.7000\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5358 - accuracy: 0.7000\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5352 - accuracy: 0.7000\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5349 - accuracy: 0.7000\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5107 - accuracy: 0.76 - 0s 8ms/step - loss: 0.5347 - accuracy: 0.7000\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5346 - accuracy: 0.7000\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5344 - accuracy: 0.7000\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5342 - accuracy: 0.7067\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5347 - accuracy: 0.7067\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5339 - accuracy: 0.7067\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5339 - accuracy: 0.7000\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5335 - accuracy: 0.7000\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5332 - accuracy: 0.7000\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5344 - accuracy: 0.7000\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5328 - accuracy: 0.7000\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5325 - accuracy: 0.7000\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5334 - accuracy: 0.7067\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5322 - accuracy: 0.7067\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5330 - accuracy: 0.7067\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5323 - accuracy: 0.7067\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5328 - accuracy: 0.7200\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5317 - accuracy: 0.7267\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5318 - accuracy: 0.7133\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.72 - 0s 8ms/step - loss: 0.5316 - accuracy: 0.7133\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5306 - accuracy: 0.7067\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5310 - accuracy: 0.7133\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5310 - accuracy: 0.7133\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5306 - accuracy: 0.7267\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5301 - accuracy: 0.7067\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5296 - accuracy: 0.7067\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5290 - accuracy: 0.7067\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5451 - accuracy: 0.70 - 0s 8ms/step - loss: 0.5294 - accuracy: 0.7200\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5293 - accuracy: 0.7133\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5291 - accuracy: 0.7067\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5300 - accuracy: 0.7067\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5291 - accuracy: 0.7133\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 530us/step - loss: 0.5281 - accuracy: 0.7133\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.68 - 0s 8ms/step - loss: 0.5283 - accuracy: 0.7267\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5279 - accuracy: 0.7333\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5274 - accuracy: 0.7200\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5278 - accuracy: 0.7133\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5281 - accuracy: 0.70 - 0s 0s/step - loss: 0.5272 - accuracy: 0.7067\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5271 - accuracy: 0.7133\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5270 - accuracy: 0.7267\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5269 - accuracy: 0.7267\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5270 - accuracy: 0.7133\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5262 - accuracy: 0.7067\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.70 - 0s 9ms/step - loss: 0.5259 - accuracy: 0.7200\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5259 - accuracy: 0.7133\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5255 - accuracy: 0.7133\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5254 - accuracy: 0.7200\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5252 - accuracy: 0.7200\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5251 - accuracy: 0.7133\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5251 - accuracy: 0.7067\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5246 - accuracy: 0.7200\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.70 - 0s 8ms/step - loss: 0.5248 - accuracy: 0.7267\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5248 - accuracy: 0.7267\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5241 - accuracy: 0.7267\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5237 - accuracy: 0.7200\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5238 - accuracy: 0.7133\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5233 - accuracy: 0.7200\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5231 - accuracy: 0.7267\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5229 - accuracy: 0.7267\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5227 - accuracy: 0.7267\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5227 - accuracy: 0.7200\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5227 - accuracy: 0.7200\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5222 - accuracy: 0.7200\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5221 - accuracy: 0.7267\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5220 - accuracy: 0.7267\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5216 - accuracy: 0.7200\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5216 - accuracy: 0.7200\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5122 - accuracy: 0.74 - 0s 13ms/step - loss: 0.5214 - accuracy: 0.7267\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.5213 - accuracy: 0.7267\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5213 - accuracy: 0.7267\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5207 - accuracy: 0.7200\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5207 - accuracy: 0.7267\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.80 - 0s 9ms/step - loss: 0.5204 - accuracy: 0.7267\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5204 - accuracy: 0.7200\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5200 - accuracy: 0.7200\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5201 - accuracy: 0.7200\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.77 - 0s 8ms/step - loss: 0.5193 - accuracy: 0.7200\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.74 - 0s 8ms/step - loss: 0.5192 - accuracy: 0.7267\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.68 - 0s 8ms/step - loss: 0.5195 - accuracy: 0.7267\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5192 - accuracy: 0.7267\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5187 - accuracy: 0.7200\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5194 - accuracy: 0.7200\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5185 - accuracy: 0.7200\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5181 - accuracy: 0.7267\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5182 - accuracy: 0.7267\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5182 - accuracy: 0.7267\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5181 - accuracy: 0.7267\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.68 - 0s 6ms/step - loss: 0.5178 - accuracy: 0.7200\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5173 - accuracy: 0.7200\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5170 - accuracy: 0.7267\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5171 - accuracy: 0.7267\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5168 - accuracy: 0.7267\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5164 - accuracy: 0.7267\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5165 - accuracy: 0.7267\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5163 - accuracy: 0.7200\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5161 - accuracy: 0.7267\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5157 - accuracy: 0.7267\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5156 - accuracy: 0.7267\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5158 - accuracy: 0.7267\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5156 - accuracy: 0.7200\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5151 - accuracy: 0.7200\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5145 - accuracy: 0.7267\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5149 - accuracy: 0.7267\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5148 - accuracy: 0.7267\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5144 - accuracy: 0.7267\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5543 - accuracy: 0.66 - 0s 0s/step - loss: 0.5141 - accuracy: 0.7267\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5139 - accuracy: 0.7267\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5137 - accuracy: 0.7267\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5137 - accuracy: 0.7267\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5135 - accuracy: 0.7267\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5133 - accuracy: 0.7267\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5133 - accuracy: 0.7267\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5130 - accuracy: 0.7333\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5127 - accuracy: 0.7267\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5125 - accuracy: 0.7267\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5123 - accuracy: 0.7400\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5121 - accuracy: 0.7267\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5126 - accuracy: 0.7333\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.5116 - accuracy: 0.7333\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5121 - accuracy: 0.7267\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5114 - accuracy: 0.7267\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5112 - accuracy: 0.7267\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.5111 - accuracy: 0.7267\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5107 - accuracy: 0.7267\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5104 - accuracy: 0.7267\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5103 - accuracy: 0.7267\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5102 - accuracy: 0.7267\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5098 - accuracy: 0.7333\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5097 - accuracy: 0.7467\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5096 - accuracy: 0.7333\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5094 - accuracy: 0.7333\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5097 - accuracy: 0.7333\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5093 - accuracy: 0.7267\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5088 - accuracy: 0.7267\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5085 - accuracy: 0.7267\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4793 - accuracy: 0.76 - 0s 11ms/step - loss: 0.5085 - accuracy: 0.7333\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5088 - accuracy: 0.7333\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5086 - accuracy: 0.7333\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5081 - accuracy: 0.7333\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5080 - accuracy: 0.7333\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5080 - accuracy: 0.7400\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5073 - accuracy: 0.7333\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5075 - accuracy: 0.7333\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5078 - accuracy: 0.7400\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5072 - accuracy: 0.7533\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5066 - accuracy: 0.7467\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5071 - accuracy: 0.7333\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5067 - accuracy: 0.7333\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5064 - accuracy: 0.7333\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5060 - accuracy: 0.7400\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5062 - accuracy: 0.7400\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7400\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5056 - accuracy: 0.7267\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5053 - accuracy: 0.7400\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5060 - accuracy: 0.7467\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5049 - accuracy: 0.7533\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5050 - accuracy: 0.7267\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5049 - accuracy: 0.7333\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5049 - accuracy: 0.7333\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5043 - accuracy: 0.7333\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5042 - accuracy: 0.7533\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5041 - accuracy: 0.7533\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5040 - accuracy: 0.7533\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5035 - accuracy: 0.7533\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5035 - accuracy: 0.7533\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5034 - accuracy: 0.7533\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5036 - accuracy: 0.7467\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5032 - accuracy: 0.7467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16abfd6dd00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"siamese\")\n",
    "model.fit([pairs_train[:,0], pairs_train[:,1]], labels_train[:], batch_size=75, epochs=150, callbacks=[WandbCallback()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
